{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa5a9339970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd # torch中自动计算梯度模块\n",
    "import torch.nn as nn             # 神经网络模块\n",
    "import torch.nn.functional as F   # 神经网络模块中的常用功能 \n",
    "import torch.optim as optim       # 模型优化器模块\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ice/Data/cbvrp-acmmm-2019-release-train-val/cbvrp_data/track_1_series'\n",
    "feature_path = '{}/train_val/features'.format(data_path)\n",
    "ith_frame_ft = feature_path + '/frame_ft/{}.npy'\n",
    "ith_audio_ft = feature_path + '/audio_ft/{}.npy'\n",
    "ith_video_ft = feature_path + '/video_ft/{}.npy'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 2048)\n",
      "(74, 2048)\n",
      "(73, 512)\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "frame_ft = np.load(ith_frame_ft.format(i))\n",
    "audio_ft = np.load(ith_audio_ft.format(i))\n",
    "video_ft = np.load(ith_video_ft.format(i))\n",
    "print(frame_ft.shape)\n",
    "print(frame_ft.shape)\n",
    "print(video_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([205., 152., 155., 153., 153., 146., 120.,  97., 100.,  74.,  76.,\n",
       "         65.,  78.,  65.,  59.,  41.,  32.,  28.,  29.,  25.,  26.,  27.,\n",
       "         18.,  21.,  10.,  15.,  10.,  10.,   8.,  10.,   6.,   9.,   4.,\n",
       "          4.,   2.,   2.,   0.,   1.,   2.,   0.,   3.,   2.,   0.,   1.,\n",
       "          0.,   2.,   0.,   1.,   0.,   1.]),\n",
       " array([0.        , 0.04319348, 0.08638696, 0.12958044, 0.17277391,\n",
       "        0.21596739, 0.25916087, 0.30235435, 0.34554783, 0.38874131,\n",
       "        0.43193479, 0.47512826, 0.51832174, 0.56151522, 0.6047087 ,\n",
       "        0.64790218, 0.69109566, 0.73428914, 0.77748261, 0.82067609,\n",
       "        0.86386957, 0.90706305, 0.95025653, 0.99345001, 1.03664349,\n",
       "        1.07983696, 1.12303044, 1.16622392, 1.2094174 , 1.25261088,\n",
       "        1.29580436, 1.33899784, 1.38219131, 1.42538479, 1.46857827,\n",
       "        1.51177175, 1.55496523, 1.59815871, 1.64135219, 1.68454566,\n",
       "        1.72773914, 1.77093262, 1.8141261 , 1.85731958, 1.90051306,\n",
       "        1.94370654, 1.98690001, 2.03009349, 2.07328697, 2.11648045,\n",
       "        2.15967393]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.hist(frame_ft[0], bins=50)\n",
    "# plt.hist(video_ft[0], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "a = [np.random.randint(22) for i in range(200)]\n",
    "\n",
    "train_data = np.array([list(range(i, i + 5)) for i in a]).reshape(-1, 1)\n",
    "\n",
    "train_data = mlb.fit_transform(train_data)\n",
    "train_data = train_data.reshape(-1, 5, train_data.shape[-1])\n",
    "\n",
    "train_X = torch.Tensor(train_data[:, :-1, :])\n",
    "train_y = torch.Tensor(train_data[:, -1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (autograd.Variable(torch.randn((self.num_layers, self.batch_size, self.hidden_dim))), autograd.Variable(torch.randn((self.num_layers, self.batch_size, self.hidden_dim))))\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         # Forward pass through LSTM layer\n",
    "#         # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "#         # shape of self.hidden: (a, b), where a and b both \n",
    "#         # have shape (num_layers, batch_size, hidden_dim).\n",
    "#         self.hidden = self.init_hidden()\n",
    "#         lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1), self.hidden)\n",
    "#         # Only take the output from the final timetep\n",
    "#         # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "#         y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        \n",
    "#         return y_pred.view(-1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.hidden = self.init_hidden()\n",
    "        for i in input:\n",
    "            lstm_out, self.hidden = self.lstm(i.view(1, self.batch_size, -1), self.hidden)\n",
    "        return self.softmax(self.linear(lstm_out.view(self.batch_size, -1)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model = LSTM(train_data.shape[-1], 16, batch_size=1, output_dim=train_data.shape[-1], num_layers=2)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss(size_average=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_pred, y_true):\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([26, 1])) that is different to the input size (torch.Size([1, 26])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.644781231880188\n",
      "Epoch  1 MSE:  0.39831146597862244\n",
      "Epoch  2 MSE:  0.23161298036575317\n",
      "Epoch  3 MSE:  0.1982080340385437\n",
      "Epoch  4 MSE:  0.18797968327999115\n",
      "Epoch  5 MSE:  0.17394894361495972\n",
      "Epoch  6 MSE:  0.17411796748638153\n",
      "Epoch  7 MSE:  0.17108510434627533\n",
      "Epoch  8 MSE:  0.17105168104171753\n",
      "Epoch  9 MSE:  0.17106667160987854\n",
      "Epoch  10 MSE:  0.17042741179466248\n",
      "Epoch  11 MSE:  0.1715407371520996\n",
      "Epoch  12 MSE:  0.1726677119731903\n",
      "Epoch  13 MSE:  0.17183491587638855\n",
      "Epoch  14 MSE:  0.17181715369224548\n",
      "Epoch  15 MSE:  0.17172002792358398\n",
      "Epoch  16 MSE:  0.17232674360275269\n",
      "Epoch  17 MSE:  0.1723284125328064\n",
      "Epoch  18 MSE:  0.1720682680606842\n",
      "Epoch  19 MSE:  0.17308726906776428\n",
      "Epoch  20 MSE:  0.17191743850708008\n",
      "Epoch  21 MSE:  0.17261026799678802\n",
      "Epoch  22 MSE:  0.17323622107505798\n",
      "Epoch  23 MSE:  0.1727355569601059\n",
      "Epoch  24 MSE:  0.17303673923015594\n",
      "Epoch  25 MSE:  0.17341110110282898\n",
      "Epoch  26 MSE:  0.17349116504192352\n",
      "Epoch  27 MSE:  0.17228204011917114\n",
      "Epoch  28 MSE:  0.17305085062980652\n",
      "Epoch  29 MSE:  0.17340460419654846\n",
      "Epoch  30 MSE:  0.17386174201965332\n",
      "Epoch  31 MSE:  0.17363902926445007\n",
      "Epoch  32 MSE:  0.17359909415245056\n",
      "Epoch  33 MSE:  0.17365604639053345\n",
      "Epoch  34 MSE:  0.17373725771903992\n",
      "Epoch  35 MSE:  0.17337661981582642\n",
      "Epoch  36 MSE:  0.1733238399028778\n",
      "Epoch  37 MSE:  0.17375344038009644\n",
      "Epoch  38 MSE:  0.17413441836833954\n",
      "Epoch  39 MSE:  0.17382170259952545\n",
      "Epoch  40 MSE:  0.17366130650043488\n",
      "Epoch  41 MSE:  0.17402972280979156\n",
      "Epoch  42 MSE:  0.1736244112253189\n",
      "Epoch  43 MSE:  0.17386123538017273\n",
      "Epoch  44 MSE:  0.17387519776821136\n",
      "Epoch  45 MSE:  0.1738378256559372\n",
      "Epoch  46 MSE:  0.17375054955482483\n",
      "Epoch  47 MSE:  0.17351427674293518\n",
      "Epoch  48 MSE:  0.17377647757530212\n",
      "Epoch  49 MSE:  0.17319585382938385\n",
      "Epoch  50 MSE:  0.17341627180576324\n",
      "Epoch  51 MSE:  0.1737685203552246\n",
      "Epoch  52 MSE:  0.1732115000486374\n",
      "Epoch  53 MSE:  0.17392462491989136\n",
      "Epoch  54 MSE:  0.172003835439682\n",
      "Epoch  55 MSE:  0.17309124767780304\n",
      "Epoch  56 MSE:  0.17326202988624573\n",
      "Epoch  57 MSE:  0.17357397079467773\n",
      "Epoch  58 MSE:  0.17262539267539978\n",
      "Epoch  59 MSE:  0.17295996844768524\n",
      "Epoch  60 MSE:  0.1716575026512146\n",
      "Epoch  61 MSE:  0.1722625344991684\n",
      "Epoch  62 MSE:  0.16867214441299438\n",
      "Epoch  63 MSE:  0.16991673409938812\n",
      "Epoch  64 MSE:  0.16834260523319244\n",
      "Epoch  65 MSE:  0.16793186962604523\n",
      "Epoch  66 MSE:  0.1636599749326706\n",
      "Epoch  67 MSE:  0.16865891218185425\n",
      "Epoch  68 MSE:  0.16458456218242645\n",
      "Epoch  69 MSE:  0.16666218638420105\n",
      "Epoch  70 MSE:  0.16546140611171722\n",
      "Epoch  71 MSE:  0.16573183238506317\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    #model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    for X, y in zip(train_X, train_y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X.reshape(-1, 1, train_data.shape[-1]))\n",
    "        loss = loss_fn(y_pred, y.reshape(-1, 1))\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in train_X:\n",
    "    y_pred = model(X.reshape(-1, 1, train_data.shape[-1]))\n",
    "    print(np.argmax(y_pred.detach().numpy()))\n",
    "    print(y_pred)\n",
    "    print(np.argmax(X.detach().numpy(), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
